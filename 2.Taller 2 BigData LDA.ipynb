{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0e12b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0-cp39-cp39-win_amd64.whl (23.9 MB)\n",
      "Collecting Cython==0.29.28\n",
      "  Downloading Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.0.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\rober\\anaconda3\\lib\\site-packages (from gensim) (1.20.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\rober\\anaconda3\\lib\\site-packages (from gensim) (1.7.1)\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.24\n",
      "    Uninstalling Cython-0.29.24:\n",
      "      Successfully uninstalled Cython-0.29.24\n",
      "Successfully installed Cython-0.29.28 gensim-4.2.0 smart-open-6.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8aa1013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rober\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pprint\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import re\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "import nltk.stem as stemmer\n",
    "stemmer = PorterStemmer()\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "569b7506",
   "metadata": {},
   "outputs": [],
   "source": [
    "File = pd.ExcelFile('DatosWoS.xlsx')\n",
    "df=File.parse('Sheet1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dc628ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>referencias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>78791.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>45.749616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>31.669358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>55.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>724.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        referencias\n",
       "count  78791.000000\n",
       "mean      45.749616\n",
       "std       31.669358\n",
       "min        0.000000\n",
       "25%       28.000000\n",
       "50%       39.000000\n",
       "75%       55.000000\n",
       "max      724.000000"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4d2cf661",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df[\"Titulo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "92c690b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = titles.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0b96d21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human versus  \n"
     ]
    }
   ],
   "source": [
    "print(text_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ae8fe5e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(text_corpus)):\n",
    "    text_corpus[i] = str(text_corpus[i])\n",
    "    text_corpus[i] = re.sub('[!,*)@#%(&$_?.^:-]', '', text_corpus[i])\n",
    "    text_corpus[i] = text_corpus[i].replace('\"','')\n",
    "    text_corpus[i] = text_corpus[i].lower()\n",
    "    text_corpus[i] = text_corpus[i].replace(\"artificial\",\"\")\n",
    "    text_corpus[i] = text_corpus[i].replace(\"intelligence\",\"\")\n",
    "print(text_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8a9abb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexs = []\n",
    "for i in range(len(text_corpus)):\n",
    "    indexs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e95a03fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(list(zip(text_corpus, indexs)),\n",
    "               columns =['Titulo', 'index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a27e89de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "efadd3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['human', 'versus', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['human', 'versu']\n"
     ]
    }
   ],
   "source": [
    "documents = df2\n",
    "doc_sample = documents[documents['index'] == 0].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "151dea92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   [human, versu]\n",
       "1                                          [brain]\n",
       "2            [test, case, studi, intellig, vehicl]\n",
       "3                              [recommend, system]\n",
       "4                           [intellectu, properti]\n",
       "5                                        [spiritu]\n",
       "6                        [emot, employe, perspect]\n",
       "7    [safe, futur, discuss, machin, ethic, safeti]\n",
       "8                  [surpass, human, factual, hoax]\n",
       "9                          [reflect, ethic, human]\n",
       "Name: Titulo, dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['Titulo'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fc329500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 human\n",
      "1 versu\n",
      "2 brain\n",
      "3 case\n",
      "4 intellig\n",
      "5 studi\n",
      "6 test\n",
      "7 vehicl\n",
      "8 recommend\n",
      "9 system\n",
      "10 intellectu\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a2180f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dafa1320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(168, 1), (236, 1), (268, 1), (393, 1), (842, 1), (1275, 1), (1343, 1)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1bbe6d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 168 (\"model\") appears 1 time.\n",
      "Word 236 (\"imag\") appears 1 time.\n",
      "Word 268 (\"covid\") appears 1 time.\n",
      "Word 393 (\"detect\") appears 1 time.\n",
      "Word 842 (\"interpret\") appears 1 time.\n",
      "Word 1275 (\"radiographi\") appears 1 time.\n",
      "Word 1343 (\"chest\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e9b4f505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5499713538886792), (1, 0.8351835186962523)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2d5d22f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d5a1631d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.027*\"optim\" + 0.027*\"model\" + 0.017*\"support\" + 0.017*\"comput\" + 0.016*\"algorithm\" + 0.016*\"decis\" + 0.015*\"predict\" + 0.013*\"base\" + 0.012*\"applic\" + 0.011*\"nonlinear\"\n",
      "Topic: 1 \n",
      "Words: 0.026*\"studi\" + 0.017*\"data\" + 0.014*\"analysi\" + 0.012*\"health\" + 0.012*\"learn\" + 0.012*\"survey\" + 0.011*\"servic\" + 0.011*\"case\" + 0.010*\"technolog\" + 0.010*\"social\"\n",
      "Topic: 2 \n",
      "Words: 0.043*\"learn\" + 0.030*\"predict\" + 0.026*\"machin\" + 0.019*\"cancer\" + 0.015*\"deep\" + 0.015*\"patient\" + 0.014*\"studi\" + 0.014*\"featur\" + 0.013*\"diseas\" + 0.012*\"model\"\n",
      "Topic: 3 \n",
      "Words: 0.026*\"problem\" + 0.019*\"represent\" + 0.018*\"graph\" + 0.013*\"solv\" + 0.012*\"knowledg\" + 0.010*\"spars\" + 0.010*\"base\" + 0.009*\"model\" + 0.008*\"human\" + 0.007*\"robot\"\n",
      "Topic: 4 \n",
      "Words: 0.027*\"recognit\" + 0.020*\"activ\" + 0.018*\"base\" + 0.016*\"imag\" + 0.015*\"model\" + 0.011*\"attent\" + 0.011*\"human\" + 0.008*\"learningbas\" + 0.008*\"multiscal\" + 0.008*\"speech\"\n",
      "Topic: 5 \n",
      "Words: 0.055*\"learn\" + 0.019*\"model\" + 0.017*\"process\" + 0.016*\"plan\" + 0.015*\"base\" + 0.014*\"reinforc\" + 0.014*\"machin\" + 0.012*\"approach\" + 0.011*\"languag\" + 0.010*\"motion\"\n",
      "Topic: 6 \n",
      "Words: 0.038*\"imag\" + 0.033*\"algorithm\" + 0.029*\"base\" + 0.024*\"learn\" + 0.023*\"detect\" + 0.019*\"classif\" + 0.017*\"deep\" + 0.017*\"featur\" + 0.013*\"cluster\" + 0.012*\"improv\"\n",
      "Topic: 7 \n",
      "Words: 0.104*\"network\" + 0.052*\"neural\" + 0.029*\"base\" + 0.027*\"detect\" + 0.019*\"convolut\" + 0.019*\"deep\" + 0.017*\"model\" + 0.014*\"learn\" + 0.010*\"classif\" + 0.010*\"predict\"\n",
      "Topic: 8 \n",
      "Words: 0.027*\"data\" + 0.018*\"analysi\" + 0.014*\"base\" + 0.014*\"method\" + 0.012*\"imag\" + 0.012*\"model\" + 0.012*\"fuzzi\" + 0.011*\"mine\" + 0.010*\"random\" + 0.009*\"classif\"\n",
      "Topic: 9 \n",
      "Words: 0.039*\"system\" + 0.038*\"control\" + 0.021*\"base\" + 0.016*\"design\" + 0.016*\"optim\" + 0.014*\"distribut\" + 0.012*\"multiag\" + 0.010*\"particl\" + 0.010*\"algorithm\" + 0.010*\"swarm\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5f58b6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.009*\"optim\" + 0.009*\"network\" + 0.007*\"algorithm\" + 0.007*\"base\" + 0.007*\"imag\" + 0.007*\"classif\" + 0.006*\"model\" + 0.006*\"analysi\" + 0.006*\"learn\" + 0.006*\"neural\"\n",
      "Topic: 1 Word: 0.010*\"network\" + 0.009*\"segment\" + 0.008*\"imag\" + 0.008*\"learn\" + 0.007*\"graph\" + 0.007*\"model\" + 0.007*\"neural\" + 0.007*\"semant\" + 0.006*\"deep\" + 0.006*\"base\"\n",
      "Topic: 2 Word: 0.008*\"learn\" + 0.007*\"base\" + 0.007*\"algorithm\" + 0.007*\"robot\" + 0.007*\"problem\" + 0.007*\"model\" + 0.006*\"optim\" + 0.006*\"time\" + 0.005*\"smart\" + 0.005*\"seri\"\n",
      "Topic: 3 Word: 0.008*\"learn\" + 0.007*\"model\" + 0.006*\"base\" + 0.006*\"network\" + 0.006*\"internet\" + 0.006*\"express\" + 0.006*\"recognit\" + 0.005*\"approach\" + 0.005*\"data\" + 0.005*\"thing\"\n",
      "Topic: 4 Word: 0.008*\"model\" + 0.007*\"predict\" + 0.006*\"base\" + 0.006*\"optim\" + 0.006*\"learn\" + 0.006*\"network\" + 0.005*\"studi\" + 0.005*\"analysi\" + 0.005*\"energi\" + 0.005*\"neural\"\n",
      "Topic: 5 Word: 0.011*\"imag\" + 0.009*\"base\" + 0.009*\"learn\" + 0.008*\"model\" + 0.008*\"network\" + 0.007*\"data\" + 0.007*\"classif\" + 0.006*\"algorithm\" + 0.006*\"deep\" + 0.006*\"predict\"\n",
      "Topic: 6 Word: 0.009*\"network\" + 0.007*\"learn\" + 0.007*\"base\" + 0.007*\"system\" + 0.007*\"fuzzi\" + 0.006*\"model\" + 0.006*\"detect\" + 0.006*\"data\" + 0.005*\"logic\" + 0.005*\"control\"\n",
      "Topic: 7 Word: 0.008*\"studi\" + 0.007*\"learn\" + 0.007*\"genom\" + 0.007*\"patient\" + 0.007*\"analysi\" + 0.006*\"clinic\" + 0.006*\"machin\" + 0.006*\"model\" + 0.006*\"diseas\" + 0.005*\"human\"\n",
      "Topic: 8 Word: 0.009*\"detect\" + 0.009*\"base\" + 0.009*\"network\" + 0.008*\"learn\" + 0.008*\"imag\" + 0.008*\"deep\" + 0.007*\"algorithm\" + 0.006*\"recognit\" + 0.006*\"optim\" + 0.006*\"neural\"\n",
      "Topic: 9 Word: 0.008*\"learn\" + 0.007*\"cancer\" + 0.007*\"network\" + 0.007*\"model\" + 0.007*\"base\" + 0.006*\"predict\" + 0.006*\"semisupervis\" + 0.005*\"deep\" + 0.005*\"quantum\" + 0.005*\"comput\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "28af623d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aidcov',\n",
       " 'interpret',\n",
       " 'model',\n",
       " 'detect',\n",
       " 'covid',\n",
       " 'chest',\n",
       " 'radiographi',\n",
       " 'imag']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a9fc8b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.443479061126709\t \n",
      "Topic: 0.038*\"imag\" + 0.033*\"algorithm\" + 0.029*\"base\" + 0.024*\"learn\" + 0.023*\"detect\" + 0.019*\"classif\" + 0.017*\"deep\" + 0.017*\"featur\" + 0.013*\"cluster\" + 0.012*\"improv\"\n",
      "\n",
      "Score: 0.3001474440097809\t \n",
      "Topic: 0.039*\"system\" + 0.038*\"control\" + 0.021*\"base\" + 0.016*\"design\" + 0.016*\"optim\" + 0.014*\"distribut\" + 0.012*\"multiag\" + 0.010*\"particl\" + 0.010*\"algorithm\" + 0.010*\"swarm\"\n",
      "\n",
      "Score: 0.1688171625137329\t \n",
      "Topic: 0.026*\"studi\" + 0.017*\"data\" + 0.014*\"analysi\" + 0.012*\"health\" + 0.012*\"learn\" + 0.012*\"survey\" + 0.011*\"servic\" + 0.011*\"case\" + 0.010*\"technolog\" + 0.010*\"social\"\n",
      "\n",
      "Score: 0.012511170469224453\t \n",
      "Topic: 0.043*\"learn\" + 0.030*\"predict\" + 0.026*\"machin\" + 0.019*\"cancer\" + 0.015*\"deep\" + 0.015*\"patient\" + 0.014*\"studi\" + 0.014*\"featur\" + 0.013*\"diseas\" + 0.012*\"model\"\n",
      "\n",
      "Score: 0.012509666383266449\t \n",
      "Topic: 0.104*\"network\" + 0.052*\"neural\" + 0.029*\"base\" + 0.027*\"detect\" + 0.019*\"convolut\" + 0.019*\"deep\" + 0.017*\"model\" + 0.014*\"learn\" + 0.010*\"classif\" + 0.010*\"predict\"\n",
      "\n",
      "Score: 0.012507534585893154\t \n",
      "Topic: 0.027*\"optim\" + 0.027*\"model\" + 0.017*\"support\" + 0.017*\"comput\" + 0.016*\"algorithm\" + 0.016*\"decis\" + 0.015*\"predict\" + 0.013*\"base\" + 0.012*\"applic\" + 0.011*\"nonlinear\"\n",
      "\n",
      "Score: 0.01250742468982935\t \n",
      "Topic: 0.027*\"recognit\" + 0.020*\"activ\" + 0.018*\"base\" + 0.016*\"imag\" + 0.015*\"model\" + 0.011*\"attent\" + 0.011*\"human\" + 0.008*\"learningbas\" + 0.008*\"multiscal\" + 0.008*\"speech\"\n",
      "\n",
      "Score: 0.012507271021604538\t \n",
      "Topic: 0.055*\"learn\" + 0.019*\"model\" + 0.017*\"process\" + 0.016*\"plan\" + 0.015*\"base\" + 0.014*\"reinforc\" + 0.014*\"machin\" + 0.012*\"approach\" + 0.011*\"languag\" + 0.010*\"motion\"\n",
      "\n",
      "Score: 0.012507034465670586\t \n",
      "Topic: 0.027*\"data\" + 0.018*\"analysi\" + 0.014*\"base\" + 0.014*\"method\" + 0.012*\"imag\" + 0.012*\"model\" + 0.012*\"fuzzi\" + 0.011*\"mine\" + 0.010*\"random\" + 0.009*\"classif\"\n",
      "\n",
      "Score: 0.01250622421503067\t \n",
      "Topic: 0.026*\"problem\" + 0.019*\"represent\" + 0.018*\"graph\" + 0.013*\"solv\" + 0.012*\"knowledg\" + 0.010*\"spars\" + 0.010*\"base\" + 0.009*\"model\" + 0.008*\"human\" + 0.007*\"robot\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ae885d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8874461054801941\t \n",
      "Topic: 0.009*\"detect\" + 0.009*\"base\" + 0.009*\"network\" + 0.008*\"learn\" + 0.008*\"imag\" + 0.008*\"deep\" + 0.007*\"algorithm\" + 0.006*\"recognit\" + 0.006*\"optim\" + 0.006*\"neural\"\n",
      "\n",
      "Score: 0.012508191168308258\t \n",
      "Topic: 0.008*\"studi\" + 0.007*\"learn\" + 0.007*\"genom\" + 0.007*\"patient\" + 0.007*\"analysi\" + 0.006*\"clinic\" + 0.006*\"machin\" + 0.006*\"model\" + 0.006*\"diseas\" + 0.005*\"human\"\n",
      "\n",
      "Score: 0.012506874278187752\t \n",
      "Topic: 0.008*\"model\" + 0.007*\"predict\" + 0.006*\"base\" + 0.006*\"optim\" + 0.006*\"learn\" + 0.006*\"network\" + 0.005*\"studi\" + 0.005*\"analysi\" + 0.005*\"energi\" + 0.005*\"neural\"\n",
      "\n",
      "Score: 0.012506592087447643\t \n",
      "Topic: 0.010*\"network\" + 0.009*\"segment\" + 0.008*\"imag\" + 0.008*\"learn\" + 0.007*\"graph\" + 0.007*\"model\" + 0.007*\"neural\" + 0.007*\"semant\" + 0.006*\"deep\" + 0.006*\"base\"\n",
      "\n",
      "Score: 0.012506299652159214\t \n",
      "Topic: 0.011*\"imag\" + 0.009*\"base\" + 0.009*\"learn\" + 0.008*\"model\" + 0.008*\"network\" + 0.007*\"data\" + 0.007*\"classif\" + 0.006*\"algorithm\" + 0.006*\"deep\" + 0.006*\"predict\"\n",
      "\n",
      "Score: 0.012505601160228252\t \n",
      "Topic: 0.009*\"network\" + 0.007*\"learn\" + 0.007*\"base\" + 0.007*\"system\" + 0.007*\"fuzzi\" + 0.006*\"model\" + 0.006*\"detect\" + 0.006*\"data\" + 0.005*\"logic\" + 0.005*\"control\"\n",
      "\n",
      "Score: 0.012505202554166317\t \n",
      "Topic: 0.008*\"learn\" + 0.007*\"model\" + 0.006*\"base\" + 0.006*\"network\" + 0.006*\"internet\" + 0.006*\"express\" + 0.006*\"recognit\" + 0.005*\"approach\" + 0.005*\"data\" + 0.005*\"thing\"\n",
      "\n",
      "Score: 0.01250515878200531\t \n",
      "Topic: 0.009*\"optim\" + 0.009*\"network\" + 0.007*\"algorithm\" + 0.007*\"base\" + 0.007*\"imag\" + 0.007*\"classif\" + 0.006*\"model\" + 0.006*\"analysi\" + 0.006*\"learn\" + 0.006*\"neural\"\n",
      "\n",
      "Score: 0.012505147606134415\t \n",
      "Topic: 0.008*\"learn\" + 0.007*\"base\" + 0.007*\"algorithm\" + 0.007*\"robot\" + 0.007*\"problem\" + 0.007*\"model\" + 0.006*\"optim\" + 0.006*\"time\" + 0.005*\"smart\" + 0.005*\"seri\"\n",
      "\n",
      "Score: 0.012504846788942814\t \n",
      "Topic: 0.008*\"learn\" + 0.007*\"cancer\" + 0.007*\"network\" + 0.007*\"model\" + 0.007*\"base\" + 0.006*\"predict\" + 0.006*\"semisupervis\" + 0.005*\"deep\" + 0.005*\"quantum\" + 0.005*\"comput\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "45854baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.49018365144729614\t Topic: 0.027*\"data\" + 0.018*\"analysi\" + 0.014*\"base\" + 0.014*\"method\" + 0.012*\"imag\"\n",
      "Score: 0.395486056804657\t Topic: 0.026*\"problem\" + 0.019*\"represent\" + 0.018*\"graph\" + 0.013*\"solv\" + 0.012*\"knowledg\"\n",
      "Score: 0.014299462549388409\t Topic: 0.026*\"studi\" + 0.017*\"data\" + 0.014*\"analysi\" + 0.012*\"health\" + 0.012*\"learn\"\n",
      "Score: 0.014295801520347595\t Topic: 0.104*\"network\" + 0.052*\"neural\" + 0.029*\"base\" + 0.027*\"detect\" + 0.019*\"convolut\"\n",
      "Score: 0.01429023128002882\t Topic: 0.038*\"imag\" + 0.033*\"algorithm\" + 0.029*\"base\" + 0.024*\"learn\" + 0.023*\"detect\"\n",
      "Score: 0.014289796352386475\t Topic: 0.027*\"optim\" + 0.027*\"model\" + 0.017*\"support\" + 0.017*\"comput\" + 0.016*\"algorithm\"\n",
      "Score: 0.01428909506648779\t Topic: 0.043*\"learn\" + 0.030*\"predict\" + 0.026*\"machin\" + 0.019*\"cancer\" + 0.015*\"deep\"\n",
      "Score: 0.014288748614490032\t Topic: 0.055*\"learn\" + 0.019*\"model\" + 0.017*\"process\" + 0.016*\"plan\" + 0.015*\"base\"\n",
      "Score: 0.014288614504039288\t Topic: 0.039*\"system\" + 0.038*\"control\" + 0.021*\"base\" + 0.016*\"design\" + 0.016*\"optim\"\n",
      "Score: 0.01428849995136261\t Topic: 0.027*\"recognit\" + 0.020*\"activ\" + 0.018*\"base\" + 0.016*\"imag\" + 0.015*\"model\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'Linguistic Query Answering on Data Cubes with Time Dimension'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dab662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

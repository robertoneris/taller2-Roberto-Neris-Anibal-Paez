{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0efe983a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install webdriver-manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dc2bc99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 103.0.5060\n",
      "[WDM] - Get LATEST chromedriver version for 103.0.5060 google-chrome\n",
      "[WDM] - About to download new driver from https://chromedriver.storage.googleapis.com/103.0.5060.53/chromedriver_win32.zip\n",
      "[WDM] - Driver has been saved in cache [C:\\Users\\rober\\.wdm\\drivers\\chromedriver\\win32\\103.0.5060.53]\n",
      "C:\\Users\\rober\\AppData\\Local\\Temp/ipykernel_6632/4164434000.py:5: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ingrese el usuario para navegaren ussrnerisn@correo.uss.cl\n",
      "ingrese la contraseña········\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import re\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "import time\n",
    "import getpass\n",
    "from bs4 import BeautifulSoup\n",
    "usuario= input(\"ingrese el usuario para navegaren uss\")\n",
    "contraseña =  getpass.getpass(\"ingrese la contraseña\")\n",
    "def if_integer(string):\n",
    "    try: \n",
    "        int(string)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "997353f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rober\\AppData\\Local\\Temp/ipykernel_6632/2281787528.py:5: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath(\"/html/body/div[4]/div/div/form/div/a\").click()\n",
      "C:\\Users\\rober\\AppData\\Local\\Temp/ipykernel_6632/2281787528.py:7: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  mBox = driver.find_element_by_xpath('//*[@id=\"i0116\"]')\n",
      "C:\\Users\\rober\\AppData\\Local\\Temp/ipykernel_6632/2281787528.py:10: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//*[@id=\"idSIButton9\"]').click()\n",
      "C:\\Users\\rober\\AppData\\Local\\Temp/ipykernel_6632/2281787528.py:12: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  mBox= driver.find_element_by_xpath('//*[@id=\"i0118\"]')\n",
      "C:\\Users\\rober\\AppData\\Local\\Temp/ipykernel_6632/2281787528.py:14: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//*[@id=\"idSIButton9\"]').click()\n",
      "C:\\Users\\rober\\AppData\\Local\\Temp/ipykernel_6632/2281787528.py:16: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//*[@id=\"idBtn_Back\"]').click()\n"
     ]
    }
   ],
   "source": [
    "#se cargan la pagina de web of science desde selenium\n",
    "driver.get(\"http://www.webofscience.com.bdigitaluss.remotexs.co/wos/woscc/summary/3a69e474-ad67-429a-baf5-12e0c77626b7-4057bdc8/relevance/1\")\n",
    "time.sleep(5)\n",
    "\n",
    "driver.find_element_by_xpath(\"/html/body/div[4]/div/div/form/div/a\").click()\n",
    "time.sleep(5)\n",
    "mBox = driver.find_element_by_xpath('//*[@id=\"i0116\"]')\n",
    "mBox.send_keys(usuario)\n",
    "time.sleep(2)\n",
    "driver.find_element_by_xpath('//*[@id=\"idSIButton9\"]').click()\n",
    "time.sleep(7)\n",
    "mBox= driver.find_element_by_xpath('//*[@id=\"i0118\"]')\n",
    "mBox.send_keys(contraseña)\n",
    "driver.find_element_by_xpath('//*[@id=\"idSIButton9\"]').click()\n",
    "time.sleep(7)\n",
    "driver.find_element_by_xpath('//*[@id=\"idBtn_Back\"]').click()\n",
    "time.sleep(10)\n",
    "driver.refresh()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4aa2d1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rober\\AppData\\Local\\Temp/ipykernel_6632/1385274439.py:1: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  total =int(driver.find_element_by_xpath('/html/body/app-wos/div/div/main/div/div[2]/app-input-route/app-base-summary-component/div/div[2]/app-page-controls[2]/div/form/div/span').text.replace(\",\",\"\"))\n",
      "C:\\Users\\rober\\AppData\\Local\\Temp/ipykernel_6632/1385274439.py:5: DeprecationWarning: find_element_by_tag_name is deprecated. Please use find_element(by=By.TAG_NAME, value=name) instead\n",
      "  htmlelement= driver.find_element_by_tag_name('html')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rober\\AppData\\Local\\Temp/ipykernel_6632/1385274439.py:20: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath(\"/html/body/app-wos/div/div/main/div/div[2]/app-input-route/app-base-summary-component/div/div[2]/app-page-controls[1]/div/form/div/button[2]\").click()\n"
     ]
    }
   ],
   "source": [
    "total =int(driver.find_element_by_xpath('/html/body/app-wos/div/div/main/div/div[2]/app-input-route/app-base-summary-component/div/div[2]/app-page-controls[2]/div/form/div/span').text.replace(\",\",\"\"))\n",
    "print(total)\n",
    "datos =[]\n",
    "driver.maximize_window()\n",
    "htmlelement= driver.find_element_by_tag_name('html')\n",
    "for i in range(total):\n",
    "    htmlelement.send_keys(Keys.END)\n",
    "    time.sleep(1)\n",
    "    htmlelement.send_keys(Keys.END)\n",
    "    time.sleep(1)\n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight/2)\")\n",
    "    time.sleep(1)\n",
    "    htmlelement.send_keys(Keys.END)\n",
    "    time.sleep(1)\n",
    "    htmlelement.send_keys(Keys.HOME)\n",
    "\n",
    "    d2 = driver.page_source\n",
    "    datos.append(d2)\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_xpath(\"/html/body/app-wos/div/div/main/div/div[2]/app-input-route/app-base-summary-component/div/div[2]/app-page-controls[1]/div/form/div/button[2]\").click()\n",
    "    time.sleep(1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fa9035e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "print(len(datos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e3d5cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data= []\n",
    "for i in range(len(datos)):\n",
    "    soup =  BeautifulSoup(datos[i])\n",
    "    for link in soup.find_all('a', attrs={'data-ta': 'summary-record-title-link'}):\n",
    "        data.append(link.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cb66924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1790\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb71cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2= []\n",
    "for i in range(len(datos)):\n",
    "    soup =  BeautifulSoup(datos[i])\n",
    "    for link in soup.find_all(attrs={'class':'citations ng-star-inserted'}):\n",
    "        data2.append(link.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0094e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(data2)):\n",
    "    msg = data2[j]\n",
    "    removeSpaces = re.sub('\\\\s+', '',msg)\n",
    "    data2[j] = removeSpaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e89667e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "966\n"
     ]
    }
   ],
   "source": [
    "print(len(data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49ca5884",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(data2)):\n",
    "    data2[i] = data2[i].replace(\"Citations\", \"\")\n",
    "    data2[i] = data2[i].replace(\"Citation\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3dfd3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rober\\AppData\\Local\\Temp/ipykernel_6632/826373608.py:1: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  tamaño =int(driver.find_element_by_xpath('/html/body/app-wos/div/div/main/div/div[2]/app-input-route/app-base-summary-component/div/div[2]/app-page-controls[2]/div/wos-select/button/span[2]').text)\n"
     ]
    }
   ],
   "source": [
    "tamaño =int(driver.find_element_by_xpath('/html/body/app-wos/div/div/main/div/div[2]/app-input-route/app-base-summary-component/div/div[2]/app-page-controls[2]/div/wos-select/button/span[2]').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12dcb043",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3= []\n",
    "for i in range(len(datos)):\n",
    "    soup =  BeautifulSoup(datos[i])\n",
    "    for k in range(tamaño):\n",
    "        for link in soup.find_all(attrs={'data-ta': 'SumAuthTa-' + str(k) + '-MainDiv-author-en'}):\n",
    "            data3.append(link.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6dbba11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1788\n"
     ]
    }
   ],
   "source": [
    "print(len(data3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf5f926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4= []\n",
    "for i in range(len(datos)):\n",
    "    soup =  BeautifulSoup(datos[i])\n",
    "    for link in soup.find_all(attrs={'data-ta':'summary-record-pubdate'}):\n",
    "        data4.append(link.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb8c136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data4)):\n",
    "    data4[i] = data4[i].replace(\"|\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f664827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "helpData4= []\n",
    "for i in range(len(datos)):\n",
    "    soup =  BeautifulSoup(datos[i])\n",
    "    for link in soup.find_all(attrs={'class':'source-info-piece ng-star-inserted'}):\n",
    "        helpData4.append(link.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78354e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(helpData4)):\n",
    "    helpData4[i] = helpData4[i].replace(\"|\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00d67421",
   "metadata": {},
   "outputs": [],
   "source": [
    "helpData5 = []\n",
    "for i in range(len(datos)):\n",
    "    soup =  BeautifulSoup(datos[i])\n",
    "    for link in soup.find_all(attrs={'class':'font-size-14 summary-source-title ng-star-inserted'}):\n",
    "        helpData5.append(link.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d1dcffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285\n"
     ]
    }
   ],
   "source": [
    "print(len(helpData5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e5475fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data5= []\n",
    "for i in range(len(datos)):\n",
    "    soup =  BeautifulSoup(datos[i])\n",
    "    for link in soup.find_all(attrs={'class':'mat-tooltip-trigger font-size-14 borderLess-button thin-focus summary-source-title-link ng-star-inserted'}):\n",
    "        data5.append(link.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8ae924d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1505\n"
     ]
    }
   ],
   "source": [
    "print(len(data5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cadbbd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data6= []\n",
    "for i in range(len(datos)):\n",
    "    soup =  BeautifulSoup(datos[i])\n",
    "    for link in soup.find_all(attrs={'class':'link-container ref-count ng-star-inserted'}):\n",
    "        data6.append(link.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "488e14b0",
   "metadata": {},
   "outputs": [],
   "source": [
    " for i in range(len(data6)):\n",
    "        data6[i] = data6[i].replace(\"References \",\"\")\n",
    "        data6[i] = data6[i].replace(\"Reference \",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c757c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations= []\n",
    "for i in range(len(datos)):\n",
    "    soup =  BeautifulSoup(datos[i])\n",
    "    for link in soup.find_all('app-record',attrs={'ng-star-inserted'}):\n",
    "        iterations.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28c899f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1790\n"
     ]
    }
   ],
   "source": [
    "print(len(iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95c3f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrappdata = []\n",
    "scrappslice = []\n",
    "countdata1 = 0\n",
    "countdata3 = 0\n",
    "countdata2 = 0\n",
    "countdata4 = 0\n",
    "countdata4help = 0\n",
    "countdata5 = 0\n",
    "countdata5help = 0\n",
    "countit = 0\n",
    "for i in range(int(len(iterations))):\n",
    "    if countit == 50:\n",
    "        countit = 0\n",
    "    scrappslice = []\n",
    "    if bool(iterations[i].find_all(attrs={'data-ta':'summary-record-pubdate'})) == True and bool(iterations[i].find_all(attrs={'class':'source-info-piece ng-star-inserted'})) == False and bool(iterations[i].find_all(attrs={'class':'mat-tooltip-trigger font-size-14 borderLess-button thin-focus summary-source-title-link ng-star-inserted'})) == True:\n",
    "        scrappslice.append(data[countdata1])\n",
    "        if if_integer(data4[countdata4]) == False:\n",
    "            dataslice = data4[countdata4].split()\n",
    "            if len(dataslice) == 2:\n",
    "                scrappslice.append(dataslice[1])\n",
    "            else:\n",
    "                scrappslice.append(dataslice[2])\n",
    "            scrappslice.append(dataslice[0])    \n",
    "        else:\n",
    "            scrappslice.append(data4[countdata4])\n",
    "            scrappslice.append(\"-\")\n",
    "        countdata4 += 1\n",
    "        if bool(iterations[i].find_all(attrs={'data-ta': 'SumAuthTa-' + str(countit) + '-MainDiv-author-en'})) == True:\n",
    "            scrappslice.append(data3[countdata3])\n",
    "            countdata3 +=1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(data5[countdata5])\n",
    "        countdata5 += 1\n",
    "        if bool(iterations[i].find_all(attrs={'class':'citations ng-star-inserted'})) == True:\n",
    "            scrappslice.append(data2[countdata2])\n",
    "            countdata2 += 1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(data6[countdata1])           \n",
    "    elif bool(iterations[i].find_all(attrs={'data-ta':'summary-record-pubdate'})) == True  and bool(iterations[i].find_all(attrs={'class':'source-info-piece ng-star-inserted'})) == False and bool(iterations[i].find_all(attrs={'class':'font-size-14 summary-source-title ng-star-inserted'})) == True:\n",
    "        scrappslice.append(data[countdata1])\n",
    "        if if_integer(data4[countdata4]) == False:\n",
    "            dataslice = data4[countdata4].split()\n",
    "            if len(dataslice) == 2:\n",
    "                scrappslice.append(dataslice[1])\n",
    "            else:\n",
    "                scrappslice.append(dataslice[2])\n",
    "            scrappslice.append(dataslice[0])    \n",
    "        else:\n",
    "            scrappslice.append(data4[countdata4])\n",
    "            scrappslice.append(\"-\")\n",
    "        countdata4 += 1\n",
    "        if bool(iterations[i].find_all(attrs={'data-ta': 'SumAuthTa-' + str(countit) + '-MainDiv-author-en'})) == True:\n",
    "            scrappslice.append(data3[countdata3])\n",
    "            countdata3 +=1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(helpData5[countdata5help])\n",
    "        countdata5help +=1\n",
    "        if bool(iterations[i].find_all(attrs={'class':'citations ng-star-inserted'})) == True:\n",
    "            scrappslice.append(data2[countdata2])\n",
    "            countdata2 += 1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(data6[countdata1])\n",
    "    elif bool(iterations[i].find_all(attrs={'data-ta':'summary-record-pubdate'})) == False and bool(iterations[i].find_all(attrs={'class':'source-info-piece ng-star-inserted'})) == True and bool(iterations[i].find_all(attrs={'class':'mat-tooltip-trigger font-size-14 borderLess-button thin-focus summary-source-title-link ng-star-inserted'})) == True:\n",
    "        scrappslice.append(data[countdata1])\n",
    "        dataslice = helpData4[countdata4help].split()\n",
    "        scrappslice.append(dataslice[1])\n",
    "        scrappslice.append(dataslice[0])\n",
    "        countdata4help += 1\n",
    "        if bool(iterations[i].find_all(attrs={'data-ta': 'SumAuthTa-' + str(countit) + '-MainDiv-author-en'})) == True:\n",
    "            scrappslice.append(data3[countdata3])\n",
    "            countdata3 +=1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(data5[countdata5])\n",
    "        countdata5 += 1\n",
    "        if bool(iterations[i].find_all(attrs={'class':'citations ng-star-inserted'})) == True:\n",
    "            scrappslice.append(data2[countdata2])\n",
    "            countdata2 += 1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(data6[countdata1])\n",
    "    elif bool(iterations[i].find_all(attrs={'data-ta':'summary-record-pubdate'})) == False and bool(iterations[i].find_all(attrs={'class':'source-info-piece ng-star-inserted'})) == True and bool(iterations[i].find_all(attrs={'class':'font-size-14 summary-source-title ng-star-inserted'})) == True:\n",
    "        scrappslice.append(data[countdata1])\n",
    "        dataslice = helpData4[countdata4help].split()\n",
    "        scrappslice.append(dataslice[1])\n",
    "        scrappslice.append(dataslice[0])\n",
    "        countdata4help += 1\n",
    "        if bool(iterations[i].find_all(attrs={'data-ta': 'SumAuthTa-' + str(countit) + '-MainDiv-author-en'})) == True:\n",
    "            scrappslice.append(data3[countdata3])\n",
    "            countdata3 +=1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(helpData5[countdata5help])\n",
    "        countdata5help += 1\n",
    "        if bool(iterations[i].find_all(attrs={'class':'citations ng-star-inserted'})) == True:\n",
    "            scrappslice.append(data2[countdata2])\n",
    "            countdata2 += 1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(data6[countdata1])\n",
    "    elif bool(iterations[i].find_all(attrs={'class':'source-info-piece ng-star-inserted'})) == True and bool(iterations[i].find_all(attrs={'data-ta':'summary-record-pubdate'})) == True and bool(iterations[i].find_all(attrs={'class':'mat-tooltip-trigger font-size-14 borderLess-button thin-focus summary-source-title-link ng-star-inserted'})) == True:\n",
    "        scrappslice.append(data[countdata1])\n",
    "        if if_integer(data4[countdata4]) == False:\n",
    "            dataslice = data4[countdata4].split()\n",
    "            if len(dataslice) == 2:\n",
    "                scrappslice.append(dataslice[1])\n",
    "            else:\n",
    "                scrappslice.append(dataslice[2])\n",
    "            scrappslice.append(dataslice[0])    \n",
    "        else:\n",
    "            scrappslice.append(data4[countdata4])\n",
    "            scrappslice.append(\"-\")\n",
    "        countdata4 += 1\n",
    "        if bool(iterations[i].find_all(attrs={'data-ta': 'SumAuthTa-' + str(countit) + '-MainDiv-author-en'})) == True:\n",
    "            scrappslice.append(data3[countdata3])\n",
    "            countdata3 +=1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(data5[countdata5])\n",
    "        countdata5 += 1\n",
    "        if bool(iterations[i].find_all(attrs={'class':'citations ng-star-inserted'})) == True:\n",
    "            scrappslice.append(data2[countdata2])\n",
    "            countdata2 += 1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(data6[countdata1])\n",
    "    elif bool(iterations[i].find_all(attrs={'class':'source-info-piece ng-star-inserted'})) == True and bool(iterations[i].find_all(attrs={'data-ta':'summary-record-pubdate'})) == True and bool(iterations[i].find_all(attrs={'class':'font-size-14 summary-source-title ng-star-inserted'})) == True: \n",
    "        scrappslice.append(data[countdata1])\n",
    "        if if_integer(data4[countdata4]) == False:\n",
    "            dataslice = data4[countdata4].split()\n",
    "            if len(dataslice) == 2:\n",
    "                scrappslice.append(dataslice[1])\n",
    "            else:\n",
    "                scrappslice.append(dataslice[2])\n",
    "            scrappslice.append(dataslice[0])    \n",
    "        else:\n",
    "            scrappslice.append(data4[countdata4])\n",
    "            scrappslice.append(\"-\")\n",
    "        countdata4 += 1\n",
    "        if bool(iterations[i].find_all(attrs={'data-ta': 'SumAuthTa-' + str(countit) + '-MainDiv-author-en'})) == True:\n",
    "            scrappslice.append(data3[countdata3])\n",
    "            countdata3 +=1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(helpData5[countdata5help])\n",
    "        countdata5help +=1\n",
    "        if bool(iterations[i].find_all(attrs={'class':'citations ng-star-inserted'})) == True:\n",
    "            scrappslice.append(data2[countdata2])\n",
    "            countdata2 += 1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(data6[countdata1])\n",
    "\n",
    "    countdata1 += 1\n",
    "    scrappdata.append(scrappslice)\n",
    "    countit += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "709c2388",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1790\n"
     ]
    }
   ],
   "source": [
    "print(len(scrappdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43e9f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "encabezados = [[\"Titulo\",\"Año\",\"Mes\",\"Autores\",\"Revista\",\"Citas\",\"referencias\"]]\n",
    "scrappdata = encabezados + scrappdata\n",
    "workbook = xlsxwriter.Workbook('DatosWoS.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "array = scrappdata\n",
    "row = 0\n",
    "\n",
    "for col, dataentry in enumerate(array):\n",
    "    worksheet.write_row(col, row, dataentry)\n",
    "\n",
    "workbook.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8eb8c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefparts = []\n",
    "for i in range(len(datos)):\n",
    "    soup =  BeautifulSoup(datos[i])\n",
    "    for link in soup.find_all('a', attrs={'data-ta': 'summary-record-title-link'}):\n",
    "        hrefparts.append(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee6c7090",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6632/1889267197.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhrefparts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http://www.webofscience.com.bdigitaluss.remotexs.co'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhrefparts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"href\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0md2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdatos2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datos2 = []\n",
    "for i in range(len(hrefparts)):\n",
    "    driver.get('http://www.webofscience.com.bdigitaluss.remotexs.co' + str(hrefparts[i].attrs[\"href\"]))\n",
    "    time.sleep(2)\n",
    "    d2 = driver.page_source\n",
    "    datos2.append(d2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bcc605cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "642\n"
     ]
    }
   ],
   "source": [
    "print(len(datos2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "97ab1e60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scrapBook = []\n",
    "for i in range(len(datos2)):\n",
    "    Keywords_Plus_list = []\n",
    "    autors = []\n",
    "    Research_Areas = []\n",
    "    scrapBookpart = []\n",
    "    n = 0\n",
    "    autorslice = \"\"\n",
    "    Keywords_Plus_list_slice = \"\"\n",
    "    Research_Areas_slice= \"\"\n",
    "    soup =  BeautifulSoup(datos2[i])\n",
    "    for link in soup.find_all(attrs={'cdxanalyticscategory':'wos-author_keyword_link'}):\n",
    "        autors.append(link.getText())\n",
    "    for link in soup.find_all(attrs={'cdxanalyticscategory':'wos-keywords-plus-link'}):\n",
    "        Keywords_Plus_list.append(link.getText())\n",
    "    for o in soup.find_all('span',attrs={'class':'value-wrap ng-star-inserted'}):\n",
    "        n += 1\n",
    "    for j in range(n):\n",
    "        for link in soup.find_all('span',attrs={'data-ta':'CategoriesTa-subject-' + str(j)}):\n",
    "            Research_Areas.append(link.getText())\n",
    "    if bool(autors) == True:\n",
    "        for m in range(len(autors)):\n",
    "            if m != 0:\n",
    "                autorslice += \";\" +autors[m] \n",
    "            else:\n",
    "                autorslice += autors[m] \n",
    "            \n",
    "        scrapBookpart.append(autorslice)\n",
    "    else:\n",
    "        scrapBookpart.append(\"-\")\n",
    "    if bool(Keywords_Plus_list) == True:\n",
    "\n",
    "        for m in range(len(Keywords_Plus_list)):\n",
    "            if m != 0:\n",
    "                Keywords_Plus_list_slice +=  \";\" + Keywords_Plus_list[m] \n",
    "            else:\n",
    "                Keywords_Plus_list_slice += Keywords_Plus_list[m] \n",
    "        scrapBookpart.append(Keywords_Plus_list_slice)\n",
    "    else:\n",
    "        scrapBookpart.append(\"-\")\n",
    "    if bool(Research_Areas) == True: \n",
    "        for m in range(len(Research_Areas)):\n",
    "            if m != 0:\n",
    "                Research_Areas_slice += \";\" + Research_Areas[m] \n",
    "            else:        \n",
    "                Research_Areas_slice += Research_Areas[m] \n",
    "        scrapBookpart.append(Research_Areas_slice) \n",
    "    else:\n",
    "        scrapBookpart.append(\"-\")\n",
    "    scrapBook.append(scrapBookpart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "b71e0d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "encabezados = [[\"Author Keywords\",\"Keywords Plus\",\"Research Areas\"]]\n",
    "scrapBook = encabezados + scrapBook\n",
    "workbook = xlsxwriter.Workbook('DatosBookWoS.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "array = scrapBook\n",
    "row = 0\n",
    "\n",
    "for col, dataentry in enumerate(array):\n",
    "    worksheet.write_row(col, row, dataentry)\n",
    "\n",
    "workbook.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dbdb84ea",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "hrefs = []\n",
    "for i in range(len(hrefparts)):\n",
    "    hrefs.append(str(hrefparts[i].attrs[\"href\"]))\n",
    "dfhref = pd.DataFrame(hrefs,columns=[\"href\"])\n",
    "dfhref.to_csv('hrefparts.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c6baf63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doctypes = []\n",
    "doctypescon = []\n",
    "for i in range(len(datos2)):\n",
    "    n = 0\n",
    "    doctypes_slice = []\n",
    "    doctypes = []\n",
    "    soup =  BeautifulSoup(datos2[i])\n",
    "    for o in soup.find_all('class',attrs={'class':'value ng-star-inserte'}):\n",
    "        n += 1\n",
    "\n",
    "    for link in soup.find_all(attrs={'data-ta':'FullRTa-doctype-' + str(n)}):\n",
    "        doctypes.append(link.getText())\n",
    "        for m in range(len(doctypes)):\n",
    "            if m != 0:\n",
    "                doctypes_slice += \";\" + doctypes[m] \n",
    "            else:\n",
    "                doctypes_slice += doctypes[m]\n",
    "        doctypescon.append(doctypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c97186bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdoctype = pd.DataFrame(doctypescon,columns=[\"tipo\"])\n",
    "dfdoctype.to_csv('Docktypes.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ccc9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "docktype.drop()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f81a3d6a",
   "metadata": {},
   "source": [
    "se cargan e instalan las librerias necesarias, ademas se inicia el selenium webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0efe983a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in c:\\users\\rober\\anaconda3\\lib\\site-packages (3.7.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\rober\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.20.0)\n",
      "Requirement already satisfied: requests in c:\\users\\rober\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.26.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rober\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\rober\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rober\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rober\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58db2c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\rober\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.3.0-py3-none-any.whl (981 kB)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\rober\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\rober\\anaconda3\\lib\\site-packages (from selenium) (0.21.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\rober\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: idna in c:\\users\\rober\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\rober\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\rober\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\rober\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\rober\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\rober\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\rober\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rober\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\rober\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\rober\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\rober\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in c:\\users\\rober\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\rober\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\rober\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\rober\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n",
      "Installing collected packages: selenium\n",
      "  Attempting uninstall: selenium\n",
      "    Found existing installation: selenium 4.2.0\n",
      "    Uninstalling selenium-4.2.0:\n",
      "      Successfully uninstalled selenium-4.2.0\n",
      "Successfully installed selenium-4.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dc2bc99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 103.0.5060\n",
      "[WDM] - Get LATEST chromedriver version for 103.0.5060 google-chrome\n",
      "[WDM] - Driver [C:\\Users\\rober\\.wdm\\drivers\\chromedriver\\win32\\103.0.5060.53\\chromedriver.exe] found in cache\n",
      "C:\\Users\\rober\\AppData\\Local\\Temp/ipykernel_15648/2412363361.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import re\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "import time\n",
    "import getpass\n",
    "from bs4 import BeautifulSoup\n",
    "#usuario= input(\"ingrese el usuario para navegar en uss \")\n",
    "#contraseña =  getpass.getpass(\"ingrese la contraseña \")\n",
    "def if_integer(string):\n",
    "    try: \n",
    "        int(string)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956256fe",
   "metadata": {},
   "source": [
    "se carga la pagina y se realiza el login automatico para iniciar el scraping, la pagina inicial se puede modificar cambiando el link dentro de drive.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "997353f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#se cargan la pagina de web of science desde selenium\n",
    "driver.get(\"http://www.webofscience.com.bdigitaluss.remotexs.co/wos/woscc/summary/3a69e474-ad67-429a-baf5-12e0c77626b7-4057bdc8/relevance/1\")\n",
    "time.sleep(3)\n",
    "\n",
    "driver.find_element(By.XPATH,\"/html/body/div[4]/div/div/form/div/a\").click()\n",
    "time.sleep(3)\n",
    "mBox = driver.find_element(By.XPATH,'//*[@id=\"i0116\"]')\n",
    "mBox.send_keys(usuario)\n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH,'//*[@id=\"idSIButton9\"]').click()\n",
    "time.sleep(3)\n",
    "mBox= driver.find_element(By.XPATH,'//*[@id=\"i0118\"]')\n",
    "mBox.send_keys(contraseña)\n",
    "time.sleep(2)\n",
    "driver.find_element(By.XPATH,'//*[@id=\"idSIButton9\"]').click()\n",
    "time.sleep(3)\n",
    "driver.find_element(By.XPATH,'//*[@id=\"idBtn_Back\"]').click()\n",
    "time.sleep(5)\n",
    "driver.refresh()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33199886",
   "metadata": {},
   "source": [
    "se extraen las paginas una a una copiando el atributo html mediante page_source y este se guarda en una lista de paginas, el numero de paginas se determina mediante total pero se puede cambiar dentro del for in range(total) a for in range(n) con n siendo el numero de paginas a extraer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4aa2d1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total =int(driver.find_element(By.XPATH,'/html/body/app-wos/div/div/main/div/div[2]/app-input-route/app-base-summary-component/div/div[2]/app-page-controls[2]/div/form/div/span').text.replace(\",\",\"\"))\n",
    "datos =[]\n",
    "driver.maximize_window()\n",
    "htmlelement= driver.find_element(By.TAG_NAME,'html')\n",
    "for i in range(total):\n",
    "    htmlelement.send_keys(Keys.END)\n",
    "    time.sleep(1)\n",
    "    htmlelement.send_keys(Keys.END)\n",
    "    time.sleep(1)\n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight/2)\")\n",
    "    time.sleep(1)\n",
    "    htmlelement.send_keys(Keys.END)\n",
    "    time.sleep(1)\n",
    "    htmlelement.send_keys(Keys.HOME)\n",
    "\n",
    "    d2 = driver.page_source\n",
    "    datos.append(d2)\n",
    "    time.sleep(1)\n",
    "    driver.find_element(By.XPATH,\"/html/body/app-wos/div/div/main/div/div[2]/app-input-route/app-base-summary-component/div/div[2]/app-page-controls[1]/div/form/div/button[2]\").click()\n",
    "    time.sleep(1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fa9035e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(datos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65395a29",
   "metadata": {},
   "source": [
    "se extraen los titulos de cada pagina almacenada en datos y se guardan en una lista data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76e3d5cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data= []\n",
    "for i in range(len(datos)):\n",
    "    soup =  BeautifulSoup(datos[i])\n",
    "    for link in soup.find_all('a', attrs={'data-ta': 'summary-record-title-link'}):\n",
    "        data.append(link.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cb66924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9fea95",
   "metadata": {},
   "source": [
    "se extraen las citas de cada pagina almacenada en datos y se guardan en una lista data2, luego esas son limpiadas de espacios y de las palabras citation y citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb71cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2= []\n",
    "for i in range(len(datos)):\n",
    "    soup =  BeautifulSoup(datos[i])\n",
    "    for link in soup.find_all(attrs={'class':'citations ng-star-inserted'}):\n",
    "        data2.append(link.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0094e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(data2)):\n",
    "    msg = data2[j]\n",
    "    removeSpaces = re.sub('\\\\s+', '',msg)\n",
    "    data2[j] = removeSpaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e89667e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n"
     ]
    }
   ],
   "source": [
    "print(len(data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49ca5884",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(data2)):\n",
    "    data2[i] = data2[i].replace(\"Citations\", \"\")\n",
    "    data2[i] = data2[i].replace(\"Citation\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acaea55",
   "metadata": {},
   "source": [
    "se obtiene el numero de articulos maximos en la pagina, luego se extraen los autores de cada articulo, estos se guardan en la lista data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12dcb043",
   "metadata": {},
   "outputs": [],
   "source": [
    "tamaño =int(driver.find_element(By.XPATH,'/html/body/app-wos/div/div/main/div/div[2]/app-input-route/app-base-summary-component/div/div[2]/app-page-controls[2]/div/wos-select/button/span[2]').text)\n",
    "data3= []\n",
    "for i in range(len(datos)):\n",
    "    soup =  BeautifulSoup(datos[i])\n",
    "    for k in range(tamaño):\n",
    "        for link in soup.find_all(attrs={'data-ta': 'SumAuthTa-' + str(k) + '-MainDiv-author-en'}):\n",
    "            data3.append(link.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6dbba11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    }
   ],
   "source": [
    "print(len(data3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2b9050",
   "metadata": {},
   "source": [
    "se obtienen las fechas de publicacion de cada articulo y se guardan en una lista data4, en este caso se creo una lista helpdata4 la cual contiene las fechas que no estan en el formato de las fechas normales, amas se limpian y guardan como fecha normal en sus listas correspondiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf5f926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4= []\n",
    "for i in range(len(datos)):\n",
    "    soup =  BeautifulSoup(datos[i])\n",
    "    for link in soup.find_all(attrs={'data-ta':'summary-record-pubdate'}):\n",
    "        data4.append(link.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb8c136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data4)):\n",
    "    data4[i] = data4[i].replace(\"|\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f664827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "helpData4= []\n",
    "for i in range(len(datos)):\n",
    "    soup =  BeautifulSoup(datos[i])\n",
    "    for link in soup.find_all(attrs={'class':'source-info-piece ng-star-inserted'}):\n",
    "        helpData4.append(link.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78354e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(helpData4)):\n",
    "    helpData4[i] = helpData4[i].replace(\"|\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c530b6",
   "metadata": {},
   "source": [
    "se obtienen los titulos de las revistas y se guardan eun una lista data5, en este caso se creo una lista helpdata5 que guarda los titulos de revistas que no contienen un link ya que estos poseen un formato diferente en la pagina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00d67421",
   "metadata": {},
   "outputs": [],
   "source": [
    "helpData5 = []\n",
    "for i in range(len(datos)):\n",
    "    soup =  BeautifulSoup(datos[i])\n",
    "    for link in soup.find_all(attrs={'class':'font-size-14 summary-source-title ng-star-inserted'}):\n",
    "        helpData5.append(link.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d1dcffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    }
   ],
   "source": [
    "print(len(helpData5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e5475fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data5= []\n",
    "for i in range(len(datos)):\n",
    "    soup =  BeautifulSoup(datos[i])\n",
    "    for link in soup.find_all(attrs={'class':'mat-tooltip-trigger font-size-14 borderLess-button thin-focus summary-source-title-link ng-star-inserted'}):\n",
    "        data5.append(link.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8ae924d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131\n"
     ]
    }
   ],
   "source": [
    "print(len(data5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a0b263",
   "metadata": {},
   "source": [
    "se extraen las referencias y se guardan en una lista data6, luego se limpian de las palabras references y reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cadbbd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data6= []\n",
    "for i in range(len(datos)):\n",
    "    soup =  BeautifulSoup(datos[i])\n",
    "    for link in soup.find_all(attrs={'class':'link-container ref-count ng-star-inserted'}):\n",
    "        data6.append(link.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "488e14b0",
   "metadata": {},
   "outputs": [],
   "source": [
    " for i in range(len(data6)):\n",
    "        data6[i] = data6[i].replace(\"References \",\"\")\n",
    "        data6[i] = data6[i].replace(\"Reference \",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981c3e22",
   "metadata": {},
   "source": [
    "se obtienen las cajas que contienen los datos de cada articulo que se extrajom de este se obtiene el numero de iteraciones para crear la lista de datos extraidos y los datos a comparar para recorrer correctamente las listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c757c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations= []\n",
    "for i in range(len(datos)):\n",
    "    soup =  BeautifulSoup(datos[i])\n",
    "    for link in soup.find_all('app-record',attrs={'ng-star-inserted'}):\n",
    "        iterations.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28c899f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    }
   ],
   "source": [
    "print(len(iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85844e51",
   "metadata": {},
   "source": [
    "se crean los datos para almacenar los datos en el formato correcto, para esto se consideran las variaciones del formato de la fecha y de la revista, ademas se comprueba si el dato existe dentro de cada caja, si existe se copia en la lista scraoodata, en caso contrario se agrega un guion mostrando que no existe dato en esa celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95c3f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrappdata = []\n",
    "scrappslice = []\n",
    "countdata1 = 0\n",
    "countdata3 = 0\n",
    "countdata2 = 0\n",
    "countdata4 = 0\n",
    "countdata4help = 0\n",
    "countdata5 = 0\n",
    "countdata5help = 0\n",
    "countit = 0\n",
    "for i in range(int(len(iterations))):\n",
    "    if countit == 50:\n",
    "        countit = 0\n",
    "    scrappslice = []\n",
    "    if bool(iterations[i].find_all(attrs={'data-ta':'summary-record-pubdate'})) == True and bool(iterations[i].find_all(attrs={'class':'source-info-piece ng-star-inserted'})) == False and bool(iterations[i].find_all(attrs={'class':'mat-tooltip-trigger font-size-14 borderLess-button thin-focus summary-source-title-link ng-star-inserted'})) == True:\n",
    "        scrappslice.append(data[countdata1])\n",
    "        if if_integer(data4[countdata4]) == False:\n",
    "            dataslice = data4[countdata4].split()\n",
    "            if len(dataslice) == 2:\n",
    "                scrappslice.append(dataslice[1])\n",
    "            else:\n",
    "                scrappslice.append(dataslice[2])\n",
    "            scrappslice.append(dataslice[0])    \n",
    "        else:\n",
    "            scrappslice.append(data4[countdata4])\n",
    "            scrappslice.append(\"-\")\n",
    "        countdata4 += 1\n",
    "        if bool(iterations[i].find_all(attrs={'data-ta': 'SumAuthTa-' + str(countit) + '-MainDiv-author-en'})) == True:\n",
    "            scrappslice.append(data3[countdata3])\n",
    "            countdata3 +=1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(data5[countdata5])\n",
    "        countdata5 += 1\n",
    "        if bool(iterations[i].find_all(attrs={'class':'citations ng-star-inserted'})) == True:\n",
    "            scrappslice.append(data2[countdata2])\n",
    "            countdata2 += 1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(data6[countdata1])           \n",
    "    elif bool(iterations[i].find_all(attrs={'data-ta':'summary-record-pubdate'})) == True  and bool(iterations[i].find_all(attrs={'class':'source-info-piece ng-star-inserted'})) == False and bool(iterations[i].find_all(attrs={'class':'font-size-14 summary-source-title ng-star-inserted'})) == True:\n",
    "        scrappslice.append(data[countdata1])\n",
    "        if if_integer(data4[countdata4]) == False:\n",
    "            dataslice = data4[countdata4].split()\n",
    "            if len(dataslice) == 2:\n",
    "                scrappslice.append(dataslice[1])\n",
    "            else:\n",
    "                scrappslice.append(dataslice[2])\n",
    "            scrappslice.append(dataslice[0])    \n",
    "        else:\n",
    "            scrappslice.append(data4[countdata4])\n",
    "            scrappslice.append(\"-\")\n",
    "        countdata4 += 1\n",
    "        if bool(iterations[i].find_all(attrs={'data-ta': 'SumAuthTa-' + str(countit) + '-MainDiv-author-en'})) == True:\n",
    "            scrappslice.append(data3[countdata3])\n",
    "            countdata3 +=1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(helpData5[countdata5help])\n",
    "        countdata5help +=1\n",
    "        if bool(iterations[i].find_all(attrs={'class':'citations ng-star-inserted'})) == True:\n",
    "            scrappslice.append(data2[countdata2])\n",
    "            countdata2 += 1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(data6[countdata1])\n",
    "    elif bool(iterations[i].find_all(attrs={'data-ta':'summary-record-pubdate'})) == False and bool(iterations[i].find_all(attrs={'class':'source-info-piece ng-star-inserted'})) == True and bool(iterations[i].find_all(attrs={'class':'mat-tooltip-trigger font-size-14 borderLess-button thin-focus summary-source-title-link ng-star-inserted'})) == True:\n",
    "        scrappslice.append(data[countdata1])\n",
    "        dataslice = helpData4[countdata4help].split()\n",
    "        scrappslice.append(dataslice[1])\n",
    "        scrappslice.append(dataslice[0])\n",
    "        countdata4help += 1\n",
    "        if bool(iterations[i].find_all(attrs={'data-ta': 'SumAuthTa-' + str(countit) + '-MainDiv-author-en'})) == True:\n",
    "            scrappslice.append(data3[countdata3])\n",
    "            countdata3 +=1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(data5[countdata5])\n",
    "        countdata5 += 1\n",
    "        if bool(iterations[i].find_all(attrs={'class':'citations ng-star-inserted'})) == True:\n",
    "            scrappslice.append(data2[countdata2])\n",
    "            countdata2 += 1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(data6[countdata1])\n",
    "    elif bool(iterations[i].find_all(attrs={'data-ta':'summary-record-pubdate'})) == False and bool(iterations[i].find_all(attrs={'class':'source-info-piece ng-star-inserted'})) == True and bool(iterations[i].find_all(attrs={'class':'font-size-14 summary-source-title ng-star-inserted'})) == True:\n",
    "        scrappslice.append(data[countdata1])\n",
    "        dataslice = helpData4[countdata4help].split()\n",
    "        scrappslice.append(dataslice[1])\n",
    "        scrappslice.append(dataslice[0])\n",
    "        countdata4help += 1\n",
    "        if bool(iterations[i].find_all(attrs={'data-ta': 'SumAuthTa-' + str(countit) + '-MainDiv-author-en'})) == True:\n",
    "            scrappslice.append(data3[countdata3])\n",
    "            countdata3 +=1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(helpData5[countdata5help])\n",
    "        countdata5help += 1\n",
    "        if bool(iterations[i].find_all(attrs={'class':'citations ng-star-inserted'})) == True:\n",
    "            scrappslice.append(data2[countdata2])\n",
    "            countdata2 += 1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(data6[countdata1])\n",
    "    elif bool(iterations[i].find_all(attrs={'class':'source-info-piece ng-star-inserted'})) == True and bool(iterations[i].find_all(attrs={'data-ta':'summary-record-pubdate'})) == True and bool(iterations[i].find_all(attrs={'class':'mat-tooltip-trigger font-size-14 borderLess-button thin-focus summary-source-title-link ng-star-inserted'})) == True:\n",
    "        scrappslice.append(data[countdata1])\n",
    "        if if_integer(data4[countdata4]) == False:\n",
    "            dataslice = data4[countdata4].split()\n",
    "            if len(dataslice) == 2:\n",
    "                scrappslice.append(dataslice[1])\n",
    "            else:\n",
    "                scrappslice.append(dataslice[2])\n",
    "            scrappslice.append(dataslice[0])    \n",
    "        else:\n",
    "            scrappslice.append(data4[countdata4])\n",
    "            scrappslice.append(\"-\")\n",
    "        countdata4 += 1\n",
    "        if bool(iterations[i].find_all(attrs={'data-ta': 'SumAuthTa-' + str(countit) + '-MainDiv-author-en'})) == True:\n",
    "            scrappslice.append(data3[countdata3])\n",
    "            countdata3 +=1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(data5[countdata5])\n",
    "        countdata5 += 1\n",
    "        if bool(iterations[i].find_all(attrs={'class':'citations ng-star-inserted'})) == True:\n",
    "            scrappslice.append(data2[countdata2])\n",
    "            countdata2 += 1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(data6[countdata1])\n",
    "    elif bool(iterations[i].find_all(attrs={'class':'source-info-piece ng-star-inserted'})) == True and bool(iterations[i].find_all(attrs={'data-ta':'summary-record-pubdate'})) == True and bool(iterations[i].find_all(attrs={'class':'font-size-14 summary-source-title ng-star-inserted'})) == True: \n",
    "        scrappslice.append(data[countdata1])\n",
    "        if if_integer(data4[countdata4]) == False:\n",
    "            dataslice = data4[countdata4].split()\n",
    "            if len(dataslice) == 2:\n",
    "                scrappslice.append(dataslice[1])\n",
    "            else:\n",
    "                scrappslice.append(dataslice[2])\n",
    "            scrappslice.append(dataslice[0])    \n",
    "        else:\n",
    "            scrappslice.append(data4[countdata4])\n",
    "            scrappslice.append(\"-\")\n",
    "        countdata4 += 1\n",
    "        if bool(iterations[i].find_all(attrs={'data-ta': 'SumAuthTa-' + str(countit) + '-MainDiv-author-en'})) == True:\n",
    "            scrappslice.append(data3[countdata3])\n",
    "            countdata3 +=1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(helpData5[countdata5help])\n",
    "        countdata5help +=1\n",
    "        if bool(iterations[i].find_all(attrs={'class':'citations ng-star-inserted'})) == True:\n",
    "            scrappslice.append(data2[countdata2])\n",
    "            countdata2 += 1\n",
    "        else:\n",
    "            scrappslice.append(\"-\")\n",
    "        scrappslice.append(data6[countdata1])\n",
    "\n",
    "    countdata1 += 1\n",
    "    scrappdata.append(scrappslice)\n",
    "    countit += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "709c2388",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    }
   ],
   "source": [
    "print(len(scrappdata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e488c303",
   "metadata": {},
   "source": [
    "se escribe en un excel los datos recogidos y ordenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "43e9f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "encabezados = [[\"Titulo\",\"Año\",\"Mes\",\"Autores\",\"Revista\",\"Citas\",\"referencias\"]]\n",
    "scrappdata = encabezados + scrappdata\n",
    "workbook = xlsxwriter.Workbook('DatosWoS.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "array = scrappdata\n",
    "row = 0\n",
    "\n",
    "for col, dataentry in enumerate(array):\n",
    "    worksheet.write_row(col, row, dataentry)\n",
    "\n",
    "workbook.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2063e853",
   "metadata": {},
   "source": [
    "se obtienen los href de cada articulo que estan almacendados como una parte del link en los atributos, luego este se guarda y ejecuta recogiendo los datos de las paginas, existe un limite de aprixmadamente 500 paginas que se pueden revisar, luego la pagina bloquea la busqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8eb8c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefparts = []\n",
    "for i in range(len(datos)):\n",
    "    soup =  BeautifulSoup(datos[i])\n",
    "    for link in soup.find_all('a', attrs={'data-ta': 'summary-record-title-link'}):\n",
    "        hrefparts.append(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee6c7090",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos2 = []\n",
    "for i in range(len(hrefparts)):\n",
    "    driver.get('http://www.webofscience.com.bdigitaluss.remotexs.co' + str(hrefparts[i].attrs[\"href\"]))\n",
    "    time.sleep(2)\n",
    "    d2 = driver.page_source\n",
    "    datos2.append(d2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bcc605cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    }
   ],
   "source": [
    "print(len(datos2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1379d439",
   "metadata": {},
   "source": [
    "se genera una lista scrapBook con los datos requeridos, se guardan dentro de la lista y se preparan para guardar en un excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97ab1e60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scrapBook = []\n",
    "for i in range(len(datos2)):\n",
    "    Keywords_Plus_list = []\n",
    "    autors = []\n",
    "    Research_Areas = []\n",
    "    scrapBookpart = []\n",
    "    n = 0\n",
    "    autorslice = \"\"\n",
    "    Keywords_Plus_list_slice = \"\"\n",
    "    Research_Areas_slice= \"\"\n",
    "    soup =  BeautifulSoup(datos2[i])\n",
    "    for link in soup.find_all(attrs={'cdxanalyticscategory':'wos-author_keyword_link'}):\n",
    "        autors.append(link.getText())\n",
    "    for link in soup.find_all(attrs={'cdxanalyticscategory':'wos-keywords-plus-link'}):\n",
    "        Keywords_Plus_list.append(link.getText())\n",
    "    for o in soup.find_all('span',attrs={'class':'value-wrap ng-star-inserted'}):\n",
    "        n += 1\n",
    "    for j in range(n):\n",
    "        for link in soup.find_all('span',attrs={'data-ta':'CategoriesTa-subject-' + str(j)}):\n",
    "            Research_Areas.append(link.getText())\n",
    "    if bool(autors) == True:\n",
    "        for m in range(len(autors)):\n",
    "            if m != 0:\n",
    "                autorslice += \";\" +autors[m] \n",
    "            else:\n",
    "                autorslice += autors[m] \n",
    "            \n",
    "        scrapBookpart.append(autorslice)\n",
    "    else:\n",
    "        scrapBookpart.append(\"-\")\n",
    "    if bool(Keywords_Plus_list) == True:\n",
    "\n",
    "        for m in range(len(Keywords_Plus_list)):\n",
    "            if m != 0:\n",
    "                Keywords_Plus_list_slice +=  \";\" + Keywords_Plus_list[m] \n",
    "            else:\n",
    "                Keywords_Plus_list_slice += Keywords_Plus_list[m] \n",
    "        scrapBookpart.append(Keywords_Plus_list_slice)\n",
    "    else:\n",
    "        scrapBookpart.append(\"-\")\n",
    "    if bool(Research_Areas) == True: \n",
    "        for m in range(len(Research_Areas)):\n",
    "            if m != 0:\n",
    "                Research_Areas_slice += \";\" + Research_Areas[m] \n",
    "            else:        \n",
    "                Research_Areas_slice += Research_Areas[m] \n",
    "        scrapBookpart.append(Research_Areas_slice) \n",
    "    else:\n",
    "        scrapBookpart.append(\"-\")\n",
    "    scrapBook.append(scrapBookpart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffeb304",
   "metadata": {},
   "source": [
    "se escribe en un excel los datos recogidos y ordenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b71e0d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "encabezados = [[\"Author Keywords\",\"Keywords Plus\",\"Research Areas\"]]\n",
    "scrapBook = encabezados + scrapBook\n",
    "workbook = xlsxwriter.Workbook('DatosBookWoS.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "array = scrapBook\n",
    "row = 0\n",
    "\n",
    "for col, dataentry in enumerate(array):\n",
    "    worksheet.write_row(col, row, dataentry)\n",
    "\n",
    "workbook.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e60828",
   "metadata": {},
   "source": [
    "se obtienen las href y se guardan en un csv por si se desea comenzar desde un archivo con los links ya extraidos, se debe de cargar el archivo y asignarlo a la lista hrefpart para poder ejecutarlod desde este punto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbdb84ea",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "hrefs = []\n",
    "for i in range(len(hrefparts)):\n",
    "    hrefs.append(str(hrefparts[i].attrs[\"href\"]))\n",
    "dfhref = pd.DataFrame(hrefs,columns=[\"href\"])\n",
    "dfhref.to_csv('hrefparts.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b3e59a",
   "metadata": {},
   "source": [
    "se recogen los tipos de documentos y se preparan para guardarlos en un csv para su uso en la parte 5 del taller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6baf63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doctypes = []\n",
    "doctypescon = []\n",
    "for i in range(len(datos2)):\n",
    "    n = 0\n",
    "    doctypes_slice = []\n",
    "    doctypes = []\n",
    "    soup =  BeautifulSoup(datos2[i])\n",
    "    for o in soup.find_all('class',attrs={'class':'value ng-star-inserte'}):\n",
    "        n += 1\n",
    "\n",
    "    for link in soup.find_all(attrs={'data-ta':'FullRTa-doctype-' + str(n)}):\n",
    "        doctypes.append(link.getText())\n",
    "        for m in range(len(doctypes)):\n",
    "            if m != 0:\n",
    "                doctypes_slice += \";\" + doctypes[m] \n",
    "            else:\n",
    "                doctypes_slice += doctypes[m]\n",
    "        doctypescon.append(doctypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbe8471",
   "metadata": {},
   "source": [
    "se escribe en un csv los datos recogidos y ordenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c97186bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdoctype = pd.DataFrame(doctypescon,columns=[\"tipo\"])\n",
    "dfdoctype.to_csv('Docktypes.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
